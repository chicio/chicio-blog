---
title: "Understanding and Leveraging Large Language Models (LLMs)"
description: "Standard transient props and shouldForwardProp are styled components API let you filter out props that 
should not be passed to the underlying React node or DOM element. In this post we will see how we can create a type 
to automatically define props from the interface of props of a parent component."
date: 2025-03-01
image: /images/posts/transient-props.png
tags: [machine learning, llm]
comments: true
commentsIdentifier: https://www.fabrizioduroni.it/2025/03/01/llm/
authors: [fabrizio_duroni]
---

Large Language Models (LLMs) are revolutionizing how we interact with technology, generating human-like text and assisting in various tasks. As a software developer at lastminute.com, I took the *Generative AI with Large Language Models* course to understand the best ways to integrate LLMs into my daily workflow, maximizing their benefits while avoiding pitfalls. I've seen cases where developers copy-paste LLM-generated code without validation, leading to erroneous or nonsensical implementations. This article serves as a structured reference of what I have learned.

## The Evolution of LLMs

Before LLMs, text generation relied on Recurrent Neural Networks (RNNs) and other structures like LSTMs. While these models improved sequential text generation, they struggled with long-term dependencies and training inefficiencies.

The game-changer came in 2017 with the paper *"Attention Is All You Need"* ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), which introduced Transformers. These models leveraged self-attention mechanisms to process input sequences in parallel, drastically improving efficiency and effectiveness.

## Transformer Architecture

Transformers consist of an **encoder-decoder** architecture:

- **Encoder**: Processes the input sequence, generating contextualized embeddings. Each encoder layer consists of self-attention and feed-forward networks. The encoder is designed to understand and capture the relationships between words, producing an enriched representation of the input.
- **Decoder**: Generates the output sequence using encoded representations and previous outputs. The decoder attends to both its own previously generated tokens and the encoded input sequence, allowing for conditioned text generation.

### Transformer Architecture Overview

Below is an illustration of the Transformer architecture:

![Transformer Model](XXXXXX)

### Key Components of Transformers

#### Self attention
Self-attention is a mechanism used in Transformer models that enables each token in an input sequence to interact with every other token, allowing the model to learn dependencies between tokens, regardless of their position in the sequence. This is particularly useful in tasks like natural language processing, where understanding the relationships between words, even if they are far apart, is crucial.

In self-attention, each token in the input sequence is transformed into three components: **query (Q)**, **key (K)**, and **value (V)**. These components are derived from the input embeddings by multiplying them by **learnable weight matrices**. Learnable weight matrices are parameters in the model that are adjusted during training to optimize the model's ability to map the input tokens to meaningful query, key, and value representations. These matrices are learned based on the data the model is exposed to, enabling the model to adapt its attention mechanism.

- **Query (Q)**: Represents the token that seeks information from other tokens.
- **Key (K)**: Represents the token that provides the information.
- **Value (V)**: Contains the actual content to be passed along based on the attention score.

The core of self-attention involves computing a score to determine how much attention one token should give to another. This score is calculated by taking the dot product of the query and key vectors for each pair of tokens. These scores are then scaled by dividing by \( \sqrt{d_k} \) (where \( d_k \) is the dimension of the key vector). This scaling helps to prevent the scores from becoming too large, which could lead to very small gradients during backpropagation, making learning difficult. After scaling, the scores are passed through a **softmax** function, which normalizes them into a probability distribution. The attention scores represent **attention patterns**, which describe the modelâ€™s strategy for deciding how much focus (or attention) each token should give to other tokens. Different attention patterns help the model learn relationships in various ways, such as identifying direct associations or more abstract connections between tokens.

The weighted sum of the value vectors, based on the attention scores, gives the output for each token. This output is a mixture of all tokens in the sequence, weighted by how much attention each token should give to the others.

Mathematically, the self-attention operation can be represented as:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Where:
- \( Q \), \( K \), and \( V \) are the query, key, and value matrices, respectively.
- \( d_k \) is the dimension of the key vector (used for scaling).

In practice, Transformers use **multi-head self-attention**, which splits the queries, keys, and values into multiple smaller "heads." Each head learns a different attention pattern, enabling the model to capture a variety of relationships simultaneously. The outputs of these multiple heads are then concatenated and projected back into the original dimensional space. The projection is done using a learnable weight matrix \( W^O \).

For example:
- One head might capture **grammatical relationships** (e.g., subject-verb agreement).
- Another head might focus on **semantic similarity** (e.g., synonyms in different contexts).
- A third head could handle **long-range dependencies** (e.g., connecting a pronoun with its antecedent in a long sentence).

Mathematically, multi-head attention is expressed as:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
$$

where each attention head is computed as:

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

Where:
- \( W_i^Q \), \( W_i^K \), and \( W_i^V \) are learnable weight matrices for each attention head. They transform the input query, key, and value matrices for each head, enabling the model to learn different representations for different heads.
- \( W^O \) is the weight matrix used to project the concatenated outputs of all the heads back into the original dimensional space.

Self-attention allows the model to process sequences in parallel and capture long-range dependencies efficiently, making it highly effective for tasks such as translation, text generation, and language modeling.
Sure! Here's an enriched version of your definitions with the additional information:

#### Positional Encoding

Since Transformers process words in parallel, **positional encoding** is used to retain the sequential order of tokens in a sequence. Without this encoding, the model would treat all tokens independently of their position in the sequence. The positional encoding is added to the token embeddings to provide the model with information about the position of each token in the sequence. This allows the model to understand the relative or absolute position of tokens, which is critical for tasks like translation or text generation.

The positional encoding is calculated as follows:

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

Where:
- \(pos\) is the position of the token in the sequence (starting from 0).
- \(i\) is the dimension index (for each element of the embedding vector).
- \(d_{model}\) is the dimensionality of the model's embeddings.

The use of **sinusoidal functions** (sine and cosine) allows the model to easily differentiate between positions in a sequence, and the choice of scaling factor \(10000^{2i/d_{model}}\) ensures that the positional encodings have a diverse set of frequencies. This enables the Transformer model to generalize well over sequences of different lengths, even if it hasn't seen certain sequence lengths during training.

#### Embeddings

**Embeddings** are dense vector representations of words or tokens that encode their semantic meaning. Each token in a sequence is mapped to a high-dimensional vector, capturing various linguistic relationships such as synonymy, analogy, and topic similarity. These embeddings are learned during training, meaning that the model adjusts them to better represent the relationships between tokens based on the data it processes.

For example, words that are semantically similar or contextually related tend to have similar embeddings. This is crucial for understanding language structures, as the embedding space captures these relationships and allows the model to make connections between words in different contexts.

In Transformer models, the embeddings serve as the initial representation of the tokens before they are processed by the self-attention mechanism. These embeddings are often combined with the positional encodings, which provide the model with information about the position of each token within the sequence.

Together, **embeddings** and **positional encodings** enable the Transformer to process sequences in parallel while retaining both semantic meaning and the order of tokens, allowing it to perform well on a variety of tasks like machine translation, text generation, and other natural language processing applications.

////// continua da qui

## Transformer Model Types

Transformers come in three main types, each optimized for different tasks:

### Encoder-only models

- **Example**: [BERT](https://arxiv.org/abs/1810.04805)
- Optimized for understanding and extracting information from text.
- Used in applications like text classification, named entity recognition, and sentiment analysis.
- **Mathematical property**: Pretrained using **masked language modeling (MLM)**, where some tokens are hidden, and the model learns to predict them.

### Decoder-only models

- **Example**: [GPT](https://cdn.openai.com/research-covers/language_models_are_fewshot_learners/paper.pdf)
- Designed for generative tasks, producing text based on input prompts.
- Used in text generation, chatbots, and code completion.
- **Mathematical property**: Trained using **causal language modeling (CLM)**, predicting the next token given previous ones:

$$
P(w_t | w_1, ..., w_{t-1})
$$

### Encoder-Decoder models

- **Example**: [T5](https://arxiv.org/abs/1910.10683), [BART](https://arxiv.org/abs/1910.13461)
- Suitable for sequence-to-sequence tasks like translation and summarization.
- The encoder processes the input while the decoder generates the output based on the encoded information.
- **Mathematical property**: Trained using **denoising autoencoders**, where parts of the input are randomly corrupted and the model learns to reconstruct them.

## Training and Fine-Tuning LLMs

### Full Pretraining

Training an LLM from scratch is highly resource-intensive but allows full control over its behavior. This involves training on massive datasets for billions of parameters. [BloombergGPT](https://arxiv.org/abs/2303.17564) is an example of a domain-specific pretrained model designed specifically for financial data analysis.

### Fine-Tuning

Fine-tuning an existing model is more efficient than full pretraining. It involves modifying the weights of an already trained model to specialize it for a specific task.

#### Parameter-Efficient Fine-Tuning (PEFT)

PEFT techniques optimize training by adjusting only a subset of weights, preventing *catastrophic forgetting* (where new training data overwrites prior knowledge). This makes fine-tuning more computationally feasible.

##### LoRA (Low-Rank Adaptation)

LoRA injects low-rank matrices into pre-trained layers, reducing the number of trainable parameters while preserving the modelâ€™s capabilities.

##### Soft Prompts

Soft prompts replace traditional text-based prompts with learned embedding vectors, enabling lightweight adaptation without altering the modelâ€™s core parameters.

*(ContinuerÃ² con ulteriori approfondimenti su RAG, ReAct, PAL e Prompt Engineering se necessario! Dimmi cosa vuoi enfatizzare di piÃ¹! ðŸš€)*

