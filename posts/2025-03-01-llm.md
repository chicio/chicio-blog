---
title: "Understanding and Leveraging Large Language Models (LLMs)"
description: "Standard transient props and shouldForwardProp are styled components API let you filter out props that 
should not be passed to the underlying React node or DOM element. In this post we will see how we can create a type 
to automatically define props from the interface of props of a parent component."
date: 2025-01-03
image: /images/posts/transient-props.png
tags: [react, typescript, web development]
comments: true
commentsIdentifier: https://www.fabrizioduroni.it/2025/01/03/styled-component-transient-props-type-mapped-type-typescript/
authors: [fabrizio_duroni]
---

Large Language Models (LLMs) are revolutionizing how we interact with technology, generating human-like text and assisting in various tasks. As a software developer at lastminute.com, I took the *Generative AI with Large Language Models* course to understand the best ways to integrate LLMs into my daily workflow, maximizing their benefits while avoiding pitfalls. I've seen cases where developers copy-paste LLM-generated code without validation, leading to erroneous or nonsensical implementations. This article serves as a structured reference of what I have learned.

## The Evolution of LLMs

Before LLMs, text generation relied on Recurrent Neural Networks (RNNs) and other structures like LSTMs. While these models improved sequential text generation, they struggled with long-term dependencies and training inefficiencies.

The game-changer came in 2017 with the paper *"Attention Is All You Need"* ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), which introduced Transformers. These models leveraged self-attention mechanisms to process input sequences in parallel, drastically improving efficiency and effectiveness.

## Transformer Architecture

Transformers consist of an **encoder-decoder** architecture:

- **Encoder**: Processes the input sequence, generating contextualized embeddings. Each encoder layer consists of self-attention and feed-forward networks. The encoder is designed to understand and capture the relationships between words, producing an enriched representation of the input.
- **Decoder**: Generates the output sequence using encoded representations and previous outputs. The decoder attends to both its own previously generated tokens and the encoded input sequence, allowing for conditioned text generation.

### Transformer Architecture Overview

Below is an illustration of the Transformer architecture:

![Transformer Model](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_1.png)

### Key Components of Transformers

#### Self-Attention Mechanism

Self-attention allows each word in a sequence to weigh its relationships with other words. The attention scores are computed as:

$$
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$

where:

- \(Q\) = Query matrix
- \(K\) = Key matrix
- \(V\) = Value matrix
- \(d_k\) = Dimensionality of keys

#### Multi-Head Attention

Instead of using a single attention mechanism, **multi-head attention** employs multiple attention heads, capturing different aspects of relationships between words. Each head learns a different type of relationship, such as syntactic dependencies, long-range dependencies, or semantic similarity. This allows the model to understand meaning at different abstraction levels.

For example:

- One head might capture **grammatical relationships** (e.g., subject-verb agreement).
- Another head might focus on **semantic similarity** (e.g., synonyms in different contexts).
- A third head could handle **long-range dependencies** (e.g., connecting a pronoun with its antecedent in a long sentence).

Mathematically, multi-head attention is expressed as:

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O
$$

where each attention head is computed as:

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

#### Positional Encoding

Since Transformers process words in parallel, **positional encoding** is used to retain sequential order:

$$
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})
$$

$$
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})
$$

where \(pos\) is the position and \(i\) is the dimension index.

### Embeddings

Embeddings are dense vector representations of words or tokens that encode semantic meaning. Each token in a sequence is mapped to a high-dimensional vector, capturing relationships such as synonymy, analogy, and topic similarity. These embeddings are learned during training and play a crucial role in understanding language structures.

## Transformer Model Types

Transformers come in three main types, each optimized for different tasks:

### Encoder-only models

- **Example**: [BERT](https://arxiv.org/abs/1810.04805)
- Optimized for understanding and extracting information from text.
- Used in applications like text classification, named entity recognition, and sentiment analysis.
- **Mathematical property**: Pretrained using **masked language modeling (MLM)**, where some tokens are hidden, and the model learns to predict them.

### Decoder-only models

- **Example**: [GPT](https://cdn.openai.com/research-covers/language_models_are_fewshot_learners/paper.pdf)
- Designed for generative tasks, producing text based on input prompts.
- Used in text generation, chatbots, and code completion.
- **Mathematical property**: Trained using **causal language modeling (CLM)**, predicting the next token given previous ones:

$$
P(w_t | w_1, ..., w_{t-1})
$$

### Encoder-Decoder models

- **Example**: [T5](https://arxiv.org/abs/1910.10683), [BART](https://arxiv.org/abs/1910.13461)
- Suitable for sequence-to-sequence tasks like translation and summarization.
- The encoder processes the input while the decoder generates the output based on the encoded information.
- **Mathematical property**: Trained using **denoising autoencoders**, where parts of the input are randomly corrupted and the model learns to reconstruct them.

## Training and Fine-Tuning LLMs

### Full Pretraining

Training an LLM from scratch is highly resource-intensive but allows full control over its behavior. This involves training on massive datasets for billions of parameters. [BloombergGPT](https://arxiv.org/abs/2303.17564) is an example of a domain-specific pretrained model designed specifically for financial data analysis.

### Fine-Tuning

Fine-tuning an existing model is more efficient than full pretraining. It involves modifying the weights of an already trained model to specialize it for a specific task.

#### Parameter-Efficient Fine-Tuning (PEFT)

PEFT techniques optimize training by adjusting only a subset of weights, preventing *catastrophic forgetting* (where new training data overwrites prior knowledge). This makes fine-tuning more computationally feasible.

##### LoRA (Low-Rank Adaptation)

LoRA injects low-rank matrices into pre-trained layers, reducing the number of trainable parameters while preserving the modelâ€™s capabilities.

##### Soft Prompts

Soft prompts replace traditional text-based prompts with learned embedding vectors, enabling lightweight adaptation without altering the modelâ€™s core parameters.

*(ContinuerÃ² con ulteriori approfondimenti su RAG, ReAct, PAL e Prompt Engineering se necessario! Dimmi cosa vuoi enfatizzare di piÃ¹! ðŸš€)*

